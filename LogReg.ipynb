{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from cis700 import tokenizer\n",
    "\n",
    "import torch\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code referenced from https://gist.github.com/gyglim/1f8dfb1b5c82627ae3efcfbbadb9f514\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.misc \n",
    "try:\n",
    "    from StringIO import StringIO  # Python 2.7\n",
    "except ImportError:\n",
    "    from io import BytesIO         # Python 3.x\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "    \n",
    "    def __init__(self, log_dir):\n",
    "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
    "        self.writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "    def scalar_summary(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\"\"\"\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "\n",
    "    def image_summary(self, tag, images, step):\n",
    "        \"\"\"Log a list of images.\"\"\"\n",
    "\n",
    "        img_summaries = []\n",
    "        for i, img in enumerate(images):\n",
    "            # Write the image to a string\n",
    "            try:\n",
    "                s = StringIO()\n",
    "            except:\n",
    "                s = BytesIO()\n",
    "            scipy.misc.toimage(img).save(s, format=\"png\")\n",
    "\n",
    "            # Create an Image object\n",
    "            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n",
    "                                       height=img.shape[0],\n",
    "                                       width=img.shape[1])\n",
    "            # Create a Summary value\n",
    "            img_summaries.append(tf.Summary.Value(tag='%s/%d' % (tag, i), image=img_sum))\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=img_summaries)\n",
    "        self.writer.add_summary(summary, step)\n",
    "        \n",
    "    def histo_summary(self, tag, values, step, bins=1000):\n",
    "        \"\"\"Log a histogram of the tensor of values.\"\"\"\n",
    "\n",
    "        # Create a histogram using numpy\n",
    "        counts, bin_edges = np.histogram(values, bins=bins)\n",
    "\n",
    "        # Fill the fields of the histogram proto\n",
    "        hist = tf.HistogramProto()\n",
    "        hist.min = float(np.min(values))\n",
    "        hist.max = float(np.max(values))\n",
    "        hist.num = int(np.prod(values.shape))\n",
    "        hist.sum = float(np.sum(values))\n",
    "        hist.sum_squares = float(np.sum(values**2))\n",
    "\n",
    "        # Drop the start of the first bin\n",
    "        bin_edges = bin_edges[1:]\n",
    "\n",
    "        # Add bin edges and counts\n",
    "        for edge in bin_edges:\n",
    "            hist.bucket_limit.append(edge)\n",
    "        for c in counts:\n",
    "            hist.bucket.append(c)\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define dataset\n",
    "class ArticleDataset(data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, articles, labels):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.articles = articles\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.articles)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        # Load data and get label\n",
    "        X = self.articles[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define classifier\n",
    "tok = tokenizer.build_tokenizer()\n",
    "\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self,num_labels,vocab_size):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)\n",
    "\n",
    "def make_bow_vector(article, word_to_ix):\n",
    "    tokens = tok.tokenize(article)\n",
    "    ids = tok.convert_tokens_to_ids(tokens)\n",
    "    words = tok.convert_ids_to_tokens(ids)\n",
    "    \n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in words:\n",
    "        if word in word_to_ix:\n",
    "            vec[word_to_ix[word]] += 1\n",
    "    return vec\n",
    "\n",
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]]).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow_vector(article, word_to_ix):\n",
    "    tokens = tok.tokenize(article)\n",
    "    ids = tok.convert_tokens_to_ids(tokens)\n",
    "    words = tok.convert_ids_to_tokens(ids)\n",
    "    \n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in words:\n",
    "        if word in word_to_ix:\n",
    "            vec[word_to_ix[word]] += 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# READ DATA ################################\n",
    "coarse_labels = open(\"coarse_labels.txt\", \"r\").readlines()\n",
    "content = open(\"content.txt\", \"r\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## CREATE COARSE CATEGORY LABEL DICT ################################\n",
    "possible_cats = open(\"supercatstats.txt\", \"r\").read().split(',')\n",
    "\n",
    "label_to_ix={}\n",
    "for line in possible_cats:\n",
    "    cat = line.split(\"'\")[1]\n",
    "    if cat not in label_to_ix:\n",
    "        label_to_ix[cat] = len(label_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## CREATE VOCAB DICT ###################################\n",
    "word_to_ix = {}\n",
    "counts = open(\"filtered_counts.txt\", \"r\").readlines()[:2000]\n",
    "for line in counts:\n",
    "    word = line.split()[1]\n",
    "    if word not in word_to_ix:\n",
    "        word_to_ix[word] = len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## CONVERT CONTENT TO VECTORS ###################################\n",
    "content_vec = [0 for _ in range(len(content))]\n",
    "print(\"converting content..\")\n",
    "for i, l in enumerate(content):\n",
    "    content_vec[i] = make_bow_vector(content[i].strip('\\n'), word_to_ix)\n",
    "    #if i%10000==0:\n",
    "    #    print(str(i)+\"/\"+str(len(content_vec)))\n",
    "print(\"done converting content..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## CONVERT LABELS TO VECTORS ###################################\n",
    "coarse_labels_vec = [0 for _ in range(len(content))]\n",
    "print(\"converting labels..\")\n",
    "for i, l in enumerate(coarse_labels):\n",
    "    coarse_labels_vec[i] = make_target(coarse_labels[i].strip('\\n'), label_to_ix)\n",
    "    \n",
    "    #if i%10000==0:\n",
    "    #    print(str(i)+\"/\"+str(len(coarse_labels_vec)))\n",
    "print(\"done converting labels..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ BUILD COARSE DATASETS ####################################\n",
    "\n",
    "torch.manual_seed(0)\n",
    "dataset = ArticleDataset(content_vec, coarse_labels_vec)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset,val_dataset,test_dataset = data.random_split(dataset,[train_size, val_size, test_size])\n",
    "\n",
    "train_loader_coarse = data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "val_loader_coarse = data.DataLoader(val_dataset, batch_size=100, shuffle=True)\n",
    "test_loader_coarse = data.DataLoader(test_dataset, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## INITIALIZE MODEL ####################################\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 180\n",
    "\n",
    "loss_function = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "import torch.nn as nn\n",
    "\n",
    "def train_model(ver=\"coarse\",num_epochs=10, lr=0.1,train_loader=train_loader_coarse,val_loader=val_loader_coarse, test_loader=test_loader_coarse):\n",
    "\n",
    "    now = time.mktime(datetime.datetime.now().timetuple())\n",
    "    logger = Logger('./logs/logreg_'+ver+'_'+str(now)+'/')\n",
    "    logger_val = Logger('./logs/logreg_val_'+ver+'_'+str(now)+'/')\n",
    "    \n",
    "    train_acc = []\n",
    "    train_loss = []\n",
    "    train_y = []\n",
    "    val_acc = []\n",
    "    val_y = []\n",
    "    val_loss = []\n",
    "    model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    total_step = len(train_loader)*num_epochs\n",
    "    step = 0\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"********EPOCH \"+str(epoch)+\"********\")\n",
    "        logs = {}\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            articles, labels = batch\n",
    "            labels = labels.view(-1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(articles)\n",
    "            _, argmax = torch.max(outputs, 1)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            accuracy = (labels == argmax).float().mean()\n",
    "            if (step + 1) % 200 == 0: \n",
    "                print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f, Accuracy: %4f'\n",
    "                  % (epoch + 1, num_epochs, step, len(train_loader) * num_epochs, loss.item(), accuracy.item())) \n",
    "                \n",
    "                to_log = {'loss': loss.item(), 'accuracy': accuracy.item()}\n",
    "                for handle, val in to_log.items():\n",
    "                    logger.scalar_summary(handle, val, step+1)\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    num_correct = 0\n",
    "                    total = 0\n",
    "                    for i, batch in enumerate(val_loader, 0):\n",
    "\n",
    "                        articles, labels = batch\n",
    "                        labels = labels.view(-1)\n",
    "                        outputs = model(articles)\n",
    "                        _, argmax = torch.max(outputs, 1)\n",
    "                        loss = loss_function(outputs, labels)\n",
    "                        num_correct += (argmax == labels).float().sum()\n",
    "                        total += articles.shape[0]\n",
    "                        accuracy = (labels == argmax).float().mean()\n",
    "                    to_log = {'loss': loss.item(), 'accuracy': accuracy.item()}\n",
    "                    for handle, val in to_log.items():\n",
    "                        print(handle, val)\n",
    "                        logger_val.scalar_summary(handle, val, step+1)\n",
    "                    print(step)\n",
    "                    print('The validation accuracy is: %s%% [%s]' % (num_correct/total * 100,100))  \n",
    "                model.train()\n",
    "\n",
    "                \n",
    "            step +=1\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            num_correct = 0\n",
    "            total = 0\n",
    "            for i, batch in enumerate(test_loader, 0):\n",
    "\n",
    "                articles, labels = batch\n",
    "                labels = labels.view(-1)\n",
    "                outputs = model(articles)\n",
    "                _, argmax = torch.max(outputs, 1)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                num_correct += (argmax == labels).float().sum()\n",
    "                total += articles.shape[0]\n",
    "                accuracy = (labels == argmax).float().mean()\n",
    "            print(step)\n",
    "            print('The test accuracy is: %s%% [%s]' % (num_correct/total * 100,100))  \n",
    "        model.train()\n",
    "        \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coarse - final model\n",
    "model_c = train_model(num_epochs=30,lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# READ FINE LABELS DATA ################################\n",
    "fine_labels = open(\"fine_labels.txt\", \"r\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## CREATE FINE CATEGORY LABEL DICT ################################\n",
    "possible_cats = open(\"catstats.txt\", \"r\").read().split(',')\n",
    "\n",
    "label_to_ix={}\n",
    "for line in possible_cats:\n",
    "    cat = line.split(\"'\")[1]\n",
    "    if cat not in label_to_ix:\n",
    "        label_to_ix[cat] = len(label_to_ix)\n",
    "\n",
    "############################## CONVERT LABELS TO VECTORS ###################################\n",
    "fine_labels_vec = [0 for _ in range(len(fine_labels))]\n",
    "print(\"converting labels..\")\n",
    "for i, l in enumerate(fine_labels):\n",
    "    fine_labels_vec[i] = make_target(fine_labels[i].strip('\\n'), label_to_ix)\n",
    "    if i%5000==0:\n",
    "        print(str(i)+\"/\"+str(len(fine_labels_vec)))\n",
    "print(\"done converting labels..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ BUILD FINE DATASETS ####################################\n",
    "dataset = ArticleDataset(content_vec, fine_labels_vec)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset,val_dataset,test_dataset = data.random_split(dataset,[train_size, val_size, test_size])\n",
    "\n",
    "train_loader_fine= data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "val_loader_fine = data.DataLoader(val_dataset, batch_size=100, shuffle=True)\n",
    "test_loader_fine = data.DataLoader(test_dataset, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 370\n",
    "\n",
    "loss_function = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine final model\n",
    "model_f = train_model(ver=\"fine\",num_epochs=20,lr=0.05,train_loader=train_loader_fine,val_loader=val_loader_fine,test_loader=test_loader_fine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
